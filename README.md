## Hi there ğŸ‘‹

<!--
**kamyarothmanhamad/kamyarothmanhamad** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ğŸ”­ Iâ€™m currently working on ...
- ğŸŒ± Iâ€™m currently learning ...
- ğŸ‘¯ Iâ€™m looking to collaborate on ...
- ğŸ¤” Iâ€™m looking for help with ...
- ğŸ’¬ Ask me about ...
- ğŸ“« How to reach me: ...
- ğŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->


<!--
  âš ï¸ Rename this file to README.md
  âš ï¸ This repoâ€™s name must exactly match your GitHub username to display this on your profile :contentReference[oaicite:3]{index=3}.
-->

# Hi there ğŸ‘‹, Iâ€™m Kamyar

**ğŸ”­ Iâ€™m currently a 2ndâ€‘year PhD student in 3D Computer Vision**  
at **CSU**, focusing on **3D human parsing** and **scene understanding**.


**ğŸŒ± Iâ€™m currently learning** advanced neural rendering techniques and geometric deep learning for deformable object modeling :contentReference[oaicite:5]{index=5}.

---

## ğŸ§‘â€ğŸ’» About Me  
- ğŸ“ **Position:** 3rdâ€‘year PhD student in 3D Computer Vision  
- ğŸ« **Affiliation:** [Your University, Department of Computer Science]  
- ğŸŒ **Website:** [yourâ€‘website.com]  
- ğŸ’¬ **Pronouns:** He/Him or She/Her or They/Them  

---

## ğŸ” Research Interests  
- **Multiâ€‘view Reconstruction:** From calibrated and uncalibrated image sets to dense 3D models :contentReference[oaicite:6]{index=6}  
- **Neural Rendering:** Implicit and explicit representations for photorealistic view synthesis  
- **Geometric Deep Learning:** Learning on meshes, point clouds, and graphs  
- **SLAM & Visual Odometry:** Robust algorithms for realâ€‘time 3D mapping  

---

## ğŸš€ Projects  
<!-- For each project, link to the repo and include a oneâ€‘sentence description. -->
- [**NeRFâ€‘3Dâ€‘Recon**](https://github.com/yourname/nerf-3d-recon): Implementation of Neural Radiance Fields for dynamic scene reconstruction with customizable control over geometry and appearance :contentReference[oaicite:7]{index=7}  
- [**Meshâ€‘Graphâ€‘CNN**](https://github.com/yourname/mesh-graph-cnn): Geometric deep network for pointâ€‘cloud segmentation and classification on benchmark datasets  
- [**SLAMâ€‘Benchmark**](https://github.com/yourname/slam-benchmark): Comparative study and evaluation suite for stateâ€‘ofâ€‘theâ€‘art SLAM approaches  
- [**3Dâ€‘Poseâ€‘Estimator**](https://github.com/yourname/3d-pose-estimator): Realâ€‘time human pose estimation and skeleton reconstruction from monocular video :contentReference[oaicite:8]{index=8}  

---

## ğŸ“š Publications  
- **Your Name**, Coâ€‘author. â€œTitle of Your ICCV/CVPR Paper,â€ *Proceedings of CVPR 2024*, pp. 1234â€“1243.  
- **Your Name**, Coâ€‘author. â€œTitle of Your ECCV/ICCV Paper,â€ *Proceedings of ICCV 2023*, pp. 567â€“578.  

---

## ğŸ› ï¸ Skills  
<details>
<summary>Click to expand</summary>

- **Languages:** Python, C++, CUDA  
- **Frameworks:** PyTorch, TensorFlow, Open3D  
- **Tools:** Git, Docker, ROS  
- **Concepts:** Photogrammetry, SLAM, Neural Rendering, Geometric DL  

</details>  
*Tip:* Use icons and badges to make this section visually appealing; see [Simple Icons](https://simpleicons.org/) for SVG badges :contentReference[oaicite:9]{index=9}.

---

## ğŸ“« Contact Me  
- ğŸ“§ Email: [your.email@university.edu]  
- ğŸ”— LinkedIn: [linkedin.com/in/yourprofile]  
- ğŸ¦ Twitter: [@yourhandle]  

---

## ğŸ”— Quick Links  
- ğŸ“„ [Curriculum Vitae](https://yourâ€‘university.edu/yourcv.pdf)  
- ğŸ“Š [GitHub Stats](https://github.com/yourname/github-readme-stats)  

---

## Tips & Best Practices  
1. **Keep it concise:** Aim for 200â€“300 linesâ€”readers skim; make each word count :contentReference[oaicite:10]{index=10}.  
2. **Use sections & emojis:** Improves scannability and conveys personality :contentReference[oaicite:11]{index=11}.  
3. **Show, donâ€™t tell:** Include GIFs/screenshots of your demos, and link to live apps if available :contentReference[oaicite:12]{index=12}.  
4. **Keep up to date:** Regularly add new publications and projects as your PhD progresses.  
